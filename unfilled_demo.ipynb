{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/jia-wei-teh/flight-fare-prediction/blob/master/unfilled_demo.ipynb) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "# handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset from csv file\n",
    "\n",
    "\n",
    "# Can also use .read_excel(). To use read_excel, it requires openpyxl (make sure you have it installed)\n",
    "# df = pd.read_excel('flights_dataset.xlsx') # uncomment if you want to use read_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the data (first 5 rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the columns in dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect info about dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rows and columns (shape)\n",
    "# ( # rows, # columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with NaN values\n",
    "# df = df.dropna() # uncomment if you want to use re-assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect info about dataframe again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata (day, month, day_of_week) from the column \"Date_of_Journey\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demo on how `to_datetime` works\n",
    "demo_date = pd.to_datetime(\"31/08/1957\", format=\"%d/%m/%Y\")\n",
    "\n",
    "demo_date.day # day of month \n",
    "demo_date.month # month     \n",
    "demo_date.year # year\n",
    "demo_date.day_of_week # Monday is 0, Sunday is 6\n",
    "demo_date.day_name() # day of week (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column Date_of_Journey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to Date_of_Journey we can extract values (hour, minute) from Dep_Time\n",
    "# Extracting Hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Dep Time column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect current dataframe with .head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to convert the Dep_Time column to minutes\n",
    "def convert_duration_to_minute(duration_string):\n",
    "    \"\"\"\n",
    "    Converts a duration string to minutes\n",
    "    \"\"\"\n",
    "    if duration_string == \"\":\n",
    "        return 0\n",
    "    else:\n",
    "        duration_list = duration_string.split(\" \") # split the string by space [ \"1h\", \"30m\" ]\n",
    "\n",
    "        if len(duration_list) != 2: \n",
    "            duration_list.append(\"0m\") if \"h\" in duration_list[0] else duration_list.insert(0, \"0h\")\n",
    "\n",
    "        hours = int(duration_list[0].split(\"h\")[0])\n",
    "        minutes = int(duration_list[1].split(\"m\")[0])\n",
    "\n",
    "        return hours*60 + minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit tests to ensure the function works\n",
    "assert convert_duration_to_minute(\"2h 30m\") == 150\n",
    "assert convert_duration_to_minute(\"1h 30m\") == 90\n",
    "assert convert_duration_to_minute(\"1h\") == 60   \n",
    "assert convert_duration_to_minute(\"30m\") == 30\n",
    "assert convert_duration_to_minute(\"\") == 0 \n",
    "\n",
    "# assert convert_duration_to_minute(\"1h 30m\") == 60 # False assertion (uncomment to see error raised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the column Dep_Time to minutes (new column Duration_min)\n",
    "# using .apply\n",
    "\n",
    "# list comprehension\n",
    "# df[\"Duration_min\"] = [ convert_duration_to_minute(duration) for duration in df[\"Duration\"] ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Duration column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of occurences of each airline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot price distribution for each airline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Airline to dummy variables (OnehotEncoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine count of each \"Source\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of price by Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Source to dummy variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Destination to dummy variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect Additional_info from dataframe \n",
    "\n",
    "\n",
    "# we observed majority of Additional_Info is 'No info'\n",
    "# Route is also too complicated to be useful (correlated to TOTAL_STOPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns \"Airline\", \"Source\", \"Destination\", \"Additional_Info\", \"Route\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out sample values (unique) in Total_Stops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using .map to convert Total_Stops to count of stops (int) - N_stop\n",
    "# mapping used: {\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns \"Total_Stops\", \"Arrival_Time\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dataframes (df , airlines, source, destination)\n",
    "\n",
    "\n",
    "# and check out the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows us to see more information regarding the DataFrame\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers (Subjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot on target variable- Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if price > 40k, then assign to median price\n",
    "\n",
    "\n",
    "# using list comprehension to assign median price to all prices > 40k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot for Target column (expected to see now more values > 40k )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Price column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Write dataframe into a csv file\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test \n",
    "# test_size: proportion of data to be used for testing\n",
    "# random_state: seed for random number generator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate the model and fit the model\n",
    "\n",
    "# calc training score\n",
    "\n",
    "\n",
    "# print training score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of model\n",
    "from sklearn.metrics import r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# print('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "# print('MSE: {}'.format(mean_squared_error(y_test, y_pred)))\n",
    "# print('RSME: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into a function with model as input \n",
    "def train_and_predict(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and predict using a given  model\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Model: \", model)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Training Score: \", model.score(X_train, y_train))\n",
    "\n",
    "    # predict on test data and it's metrics\n",
    "    predict_with_metrics(model, X_test, y_test)\n",
    "\n",
    "\n",
    "def predict_with_metrics(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Predict on test data and print metrics\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2score = r2_score(y_test, y_pred)\n",
    "    print(\"R2 Score: \", r2score)\n",
    "\n",
    "    print('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "    print('MSE: {}'.format(mean_squared_error(y_test, y_pred)))\n",
    "    print('RSME: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance\n",
    "feat_importance = pd.Series(reg_rf.feature_importances_ , index = X_train.columns)\n",
    "\n",
    "# plot top 20 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBoost Regressor \n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance (with xgboost)\n",
    "from xgboost import plot_importance \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Generally there's two types of hyperparameter tuning methods: \n",
    "- RandomizedSearchCV (Faster)\n",
    "- GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "\n",
    "param_grid= {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'subsample': [ 0.5, 0.75 , 0.9] , \n",
    "    'colsample_bytree': [ 0.5, 0.75, 0.9 ] ,\n",
    "}\n",
    "\n",
    "# estimator : The estimator being fit, here, it's the XGBoost\n",
    "# param_distribution : distributoon of the possible hyper-param\n",
    "# cv : number of cross-validation. iteration\n",
    "# n_iter : number of hyperparam combination to choose from\n",
    "# verbose: (2) print more output \n",
    "\n",
    "# xgb_model_tuned = RandomizedSearchCV( .... ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training data (with hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view best param based on the combination above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data and it's metrics\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b94be064a017ef4ea55ea8ebf8455ec65dd8bda815a5ca919074ccd68019a4b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('flight-fare')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
